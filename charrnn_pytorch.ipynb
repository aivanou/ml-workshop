{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Name generation using Recurrent neural networks\n",
    "\n",
    "In this excersise we will be implementing a machine learning algorithm that generates english names. For this we will be using Recurrent neural networks\n",
    "\n",
    "The framework that we are going to use is pytorch: [Pytorch](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "We will be implementing character level model that predicts next character given first N characters. First, we will transform raw text names inot numeric data, then we will normalize it(add padding), then we will build our character-level model, and train it on the normalized input. After that we will try to generate names. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import pytorch https://pytorch.org/docs/stable/index.html\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Import numpy https://docs.scipy.org/doc/numpy/dev/\n",
    "import numpy as np\n",
    "from random import sample\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Util function to keep track of execution time\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download task data, it consists of a single file: names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File names/names is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import common.workshop\n",
    "\n",
    "common.workshop.download_name_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art\t\t\t    NER-pytorch.ipynb\r\n",
      "artistic_transfer.ipynb     Readme\r\n",
      "charrnn_pytorch.ipynb\t    tag-prediction\r\n",
      "common\t\t\t    TagsPrediction.ipynb\r\n",
      "data\t\t\t    text-generation\r\n",
      "images\t\t\t    text_generation_pytorch.ipynb\r\n",
      "MNIST-classification.ipynb  text_generation_pytorch_old.ipynb\r\n",
      "names\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FILE = './names/names'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a method that reads data from DATA_FILE into array of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement\n",
    "\n",
    "def read_names(filepath):\n",
    "    names = []\n",
    "    with open(DATA_FILE) as f:\n",
    "        names = f.read().split('\\n')[:-1]\n",
    "    return names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "names = read_names(DATA_FILE)\n",
    "\n",
    "print(names[0:10])\n",
    "\n",
    "assert(len(names) == 7944)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain overall approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define constants\n",
    "\n",
    "MAX_VECTOR_LEN = 15\n",
    "EOS_token = '#'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function that constucts the alphabet. \n",
    "Don't forget to include EOS_token into the alphabet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement\n",
    "\n",
    "def construct_alphabet(names):\n",
    "    alphabet = set()\n",
    "    for name in names:\n",
    "        for token in name:\n",
    "            alphabet.add(token)\n",
    "    alphabet.add(EOS_token)\n",
    "    return alphabet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet variable should be a set of all unique characters that are in the input data(names)\n",
    "alphabet = construct_alphabet(names)\n",
    "n_letters = len(alphabet)\n",
    "\n",
    "assert(n_letters == 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step that we need to do is to construct two dictionaries:\n",
    "\n",
    "    token2ixdx = token -> index - is a dictionary that maps character to the idividual integer\n",
    "    \n",
    "    idx2token = index -> token - is a dictionary that maps integer back into the character\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: implement\n",
    "\n",
    "def construct_data_dictionaries(alphabet):\n",
    "    token2idx = {}\n",
    "    idx2token = {}\n",
    "    for ind,token in enumerate(alphabet):\n",
    "        token2idx[token] = ind\n",
    "        idx2token[ind] = token\n",
    "    return token2idx, idx2token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R': 0, 'T': 1, 'J': 2, 'h': 3, 'g': 4, 'D': 5, 'L': 6, 'n': 7, 'a': 8, 'q': 9, 'C': 10, 'k': 11, 'E': 12, 'O': 13, 'K': 14, 'p': 15, 'z': 16, 'Y': 17, 'm': 18, 'I': 19, \"'\": 20, 'Z': 21, 'j': 22, 'N': 23, 'd': 24, 'X': 25, '-': 26, 'U': 27, 'B': 28, 'F': 29, 'W': 30, 'y': 31, 's': 32, 'S': 33, 'A': 34, 'H': 35, 'u': 36, ' ': 37, 'b': 38, 'e': 39, 'c': 40, 'o': 41, 'G': 42, 'P': 43, 'x': 44, 'i': 45, 'r': 46, 't': 47, 'f': 48, 'Q': 49, '#': 50, 'w': 51, 'v': 52, 'M': 53, 'l': 54, 'V': 55}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token2idx, idx2token = construct_data_dictionaries(alphabet)\n",
    "\n",
    "assert(len(token2idx) == len(idx2token))\n",
    "\n",
    "assert(len(token2idx) == 56)\n",
    "\n",
    "print(token2idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding.\n",
    "\n",
    "In the following section we will implement one hot encoding. The best way to explain one hot encoding is via example:\n",
    "\n",
    "Input:\n",
    "\n",
    "    dictionary = {'t': 0, 'e': 1, 's': 2}\n",
    "    \n",
    "    input = 'test'\n",
    "    \n",
    "    output = [\n",
    "              [1, 0, 0], # t - has index 0\n",
    "              [0, 1, 0], # e - has index 1\n",
    "              [0, 0, 1], # s - has index 2\n",
    "              [1, 0, 0]  # t - has index 0\n",
    "             ]\n",
    "             \n",
    "As can be seen, one hot encoding accepts dictionary and input_string as input parameters and returns the matrix, where each row has size of the dictionary with only one index that has value 0 and all other indexes have value zero.\n",
    "\n",
    "\n",
    "In the cell below you need to implement the one_hot_encode method, given line it contructs the matrix in the form above.\n",
    "\n",
    "use token2idx as dictionary, \n",
    "\n",
    "use n_letters as size of your vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement\n",
    "\n",
    "def one_hot_encode(line):\n",
    "    matrix = np.zeros((len(line), n_letters))\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        matrix[li][token2idx[letter]] = 1\n",
    "    return matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded = one_hot_encode('test')\n",
    "\n",
    "print(encoded)\n",
    "\n",
    "assert(encoded.shape == (4, n_letters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are convenience functions for generating input data.\n",
    "\n",
    "get_target_vector returns the target output, which is shifted by one the input name with EOS token at the end.\n",
    "\n",
    "    EOS_TOKEN = '#'\n",
    "\n",
    "    dictionary = {'m': 0, 'a': 1, 'c': 2, 'h': 3, 'i': 4, 'n': 5, 'e': 6 , '#': 7}\n",
    "\n",
    "    input = machine\n",
    "    \n",
    "    output = [1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_input_vector(line):\n",
    "    return one_hot_encode(line)\n",
    "\n",
    "def get_target_vector(line):\n",
    "    letter_indexes = [token2idx[letter] for letter in line[1:]]\n",
    "    letter_indexes.append(token2idx[EOS_token]) # EOS\n",
    "    return np.array(letter_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45,  7, 39, 50])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_target_vector('line')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing that we need to do is to implement a function that given list of names, picks *batch_size* names at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement, use sample function from random package\n",
    "\n",
    "def sample_names(batch_size=32):\n",
    "    return sample(names, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zeb',\n",
       " 'Sid',\n",
       " 'Ruth',\n",
       " 'Aylmer',\n",
       " 'Gabe',\n",
       " 'Toinette',\n",
       " 'Henderson',\n",
       " 'Pamella',\n",
       " 'Lloyd',\n",
       " 'Sybyl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_names(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we defined out input and output parameters. Each train sample consists of a pair: Name, Name shifted by one. \n",
    "Using function above we will transform it to the format, understandable to neural networks. \n",
    "\n",
    "Example:\n",
    "\n",
    "    dictionary: {'m': 0, 'a': 1, 'c': 2, 'h': 3, 'i': 4, 'n': 5, 'e': 6 , '#': 7, 'z': 8}\n",
    "    \n",
    "    input:\n",
    "        x: machine - input \n",
    "        y: achine# - what we need to predict\n",
    "    \n",
    "    \n",
    "    transformed input: (get_input_vector)\n",
    "    \n",
    "        x: [\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0],  m\n",
    "            [0, 1, 0, 0, 0, 0, 0, 0],  a\n",
    "            [0, 0, 1, 0, 0, 0, 0, 0],  c\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0],  h\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0],  i\n",
    "            [0, 0, 0, 0, 0, 1, 0, 0],  n\n",
    "            [0, 0, 0, 0, 0, 0, 1, 0]   e   \n",
    "        ]\n",
    "        \n",
    "       (get_target_vector)\n",
    "       \n",
    "       y: [1,2,3,4,5,6,7] achine#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions that transform numpy arrays into pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numpy_to_tensor(numpy_matrix):\n",
    "    return torch.from_numpy(numpy_matrix)\n",
    "\n",
    "def construct_input_tensor(line):\n",
    "    input_matrix = get_input_vector(line)\n",
    "    input_tensor = numpy_to_tensor(input_matrix)\n",
    "    input_tensor = input_tensor.unsqueeze(dim = 1)\n",
    "    input_tensor = input_tensor.type(torch.FloatTensor)\n",
    "    return input_tensor\n",
    "\n",
    "def construct_output_tensor(line):\n",
    "    output_vector = get_target_vector(line)\n",
    "    output_tensor = numpy_to_tensor(output_vector)\n",
    "    output_tensor = output_tensor.type(torch.LongTensor)\n",
    "    return output_tensor\n",
    "\n",
    "def to_input_format(name):\n",
    "    i_tensor = construct_input_tensor(name)\n",
    "    o_tensor = construct_output_tensor(name)\n",
    "    \n",
    "    return i_tensor, o_tensor\n",
    "\n",
    "    \n",
    "def random_sample():\n",
    "    name = sample_names(1)[0]\n",
    "    return to_input_format(name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Intro to RNNs\n",
    "\n",
    "In the cell below is described the simple recursive neural network implemented in Pytorch.\n",
    "\n",
    "The recursive network is the extension of the linear network, developed to solve problem that involve sequential data(e.g. text). The brief schema is presented below:\n",
    "\n",
    "![title](images/rnn1.png)\n",
    "\n",
    "In the picture above the red rectangle represents the input parameters, the green rectangle represents the interanls of RNN(hidden layer) and the blue rectangle represents the output parameters. \n",
    "\n",
    "Lest walk through the formulas to understand the RNN:\n",
    "\n",
    "![title](images/rnn1_ht.png)\n",
    "\n",
    "The formula above combines the input data on timestamp(t) with the state of the hidden outputs on timestamp(t-1) and performs RELU function. RELU is a [rectified linear unit](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7) function which makes sure that the RNN is not a linear combination of vectors.\n",
    "\n",
    "The second part is to produce outputs(y) on each timestamp. It is done via:\n",
    "\n",
    "![title](images/rnn1_y.png)\n",
    "\n",
    "Since RNN is operates over time series data, during execution it unfolds into:\n",
    "\n",
    "![title](images/rnn_unfolded.png)\n",
    "\n",
    "\n",
    "In our case, on each timestamp we output a vector of length(alphabet), which corresponds to the scores of each letter in alphabet. Note: In order to find out the probability distributions of the letters, we need to apply *softmax* operation.[Softmax](https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d)\n",
    "\n",
    "It is much clear with the example. \n",
    "\n",
    "    dictionary: {'t':0, 'e': 1, 's': 2, '#': 3}\n",
    "    \n",
    "    input:  test\n",
    "    target: est#\n",
    "    \n",
    "    input_one_hot_encoded: [[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,0,0,0]]\n",
    "    target_encoded: [[1,2,0,3]]\n",
    "    \n",
    "    hidden_size: 4 , hidden_vector: [0,0,0,0]\n",
    "    \n",
    "    timestamp1:\n",
    "    \n",
    "    input: [input_one_hot_encoded, hidden_vector] \n",
    "    output: [0.1,-0.3, 0.8, -0.1]\n",
    "    next_hidden: [0.3,-0.4, 0.1, -0.7]\n",
    "\n",
    "\n",
    "Additional resources:\n",
    "\n",
    "[IntroToRNN](https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build simple RNN using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.i2h = nn.Linear(n_input + n_hidden, n_hidden)\n",
    "        self.h2o = nn.Linear(n_hidden, n_output)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        next_hidden = self.i2h(combined)\n",
    "        next_hidden = self.relu(next_hidden)\n",
    "        output = self.h2o(next_hidden)\n",
    "#         return next_hidden, self.log_softmax(output)\n",
    "        return next_hidden, output\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.n_hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to play with the network defined above.\n",
    "\n",
    "Try to specify different hidden_size and see what happens.\n",
    "\n",
    "Print input and output parameters. Try to understand why do they look like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "rnn = SimpleRNN(n_letters, hidden_size, n_letters)\n",
    "\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "input_tensor, output_tensor = random_sample()\n",
    "\n",
    "next_hidden, output = rnn(input_tensor[0], hidden)\n",
    "\n",
    "print(next_hidden.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the loss function. [CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "\n",
    "Try to play with the parameters and try to understand what are the input and output parameters of the loss_function\n",
    "\n",
    "use pdf.set_trace for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9095, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "input_tensor, target_tensor = random_sample()\n",
    "\n",
    "# \n",
    "target_tensor = target_tensor.unsqueeze_(-1)\n",
    "\n",
    "next_hidden, output_layer = rnn(input_tensor[0], rnn.initHidden())\n",
    "\n",
    "loss_function(output_layer, target_tensor[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the initialisation of the rnn that we will be using for training.\n",
    "\n",
    "In order to understand better, change the learning rate and hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128 \n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "rnn = SimpleRNN(n_letters, hidden_size, n_letters)\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_on_random_sample():\n",
    "    input_tensor, target_tensor = random_sample()\n",
    "    target_tensor = target_tensor.unsqueeze_(-1)\n",
    "    hidden = rnn.initHidden()\n",
    "    nletters = input_tensor.size(0)\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(nletters):\n",
    "        next_hidden, output = rnn(input_tensor[i], hidden)\n",
    "        l = loss_function(output, target_tensor[i])\n",
    "        hidden = next_hidden\n",
    "        loss+=l\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "    \n",
    "    return output, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 14s (5000 50%) loss: 15.2072, total_loss: 6752.2652\n",
      "0m 27s (10000 100%) loss: 10.7680, total_loss: 6517.3490\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_iters = 10000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for ind in range(1, n_iters + 1):\n",
    "    output, loss = train_on_random_sample()\n",
    "    total_loss += loss\n",
    "\n",
    "    if ind % print_every == 0:\n",
    "        print('%s (%d %d%%) loss: %.4f, total_loss: %.4f' % (timeSince(start), ind, ind / n_iters * 100, loss, total_loss))\n",
    "\n",
    "    if ind % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 15\n",
    "def sample_from_nn(start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        input = construct_input_tensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            hidden, output = rnn(input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = idx2token[topi.item()]\n",
    "                output_name += letter\n",
    "            input = construct_input_tensor(letter)\n",
    "        return output_name\n",
    "\n",
    "import pdb\n",
    "    \n",
    "def sample_from_nn_distr(start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        input = construct_input_tensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            hidden, output = rnn(input[0], hidden)\n",
    "#             pdb.set_trace()\n",
    "            output = F.softmax(output, dim = 1)\n",
    "            token_id = torch.multinomial(output[-1], 1).item()\n",
    "            letter = idx2token[token_id]\n",
    "            output_name += letter    \n",
    "            input = construct_input_tensor(start_letter)\n",
    "\n",
    "        return output_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_from_nn_distr('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3_rl)",
   "language": "python",
   "name": "conda_python3_rl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
