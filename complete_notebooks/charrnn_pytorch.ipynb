{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Name generation using Recurrent neural networks\n",
    "\n",
    "In this excersise we will be implementing a machine learning algorithm that generates english names. For this we will be using Recurrent neural networks\n",
    "\n",
    "The framework that we are going to use is pytorch: [Pytorch](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "We will be implementing character level model that predicts next character given first N characters. First, we will transform raw text names inot numeric data, then we will normalize it(add padding), then we will build our character-level model, and train it on the normalized input. After that we will try to generate names. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import pytorch https://pytorch.org/docs/stable/index.html\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Import numpy https://docs.scipy.org/doc/numpy/dev/\n",
    "import numpy as np\n",
    "from random import sample\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Util function to keep track of execution time\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download task data, it consists of a single file: names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File names/names is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import common.workshop\n",
    "\n",
    "common.workshop.download_name_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artistic_transfer.ipynb     names\t\t  text_generation_pytorch.ipynb\r\n",
      "charrnn_pytorch.ipynb\t    NER-pytorch.ipynb\r\n",
      "MNIST-classification.ipynb  TagsPrediction.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FILE = './names/names'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a method that reads data from DATA_FILE into array of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement\n",
    "\n",
    "def read_names(filepath):\n",
    "    names = []\n",
    "    with open(DATA_FILE) as f:\n",
    "        names = f.read().split('\\n')[:-1]\n",
    "    return names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "names = read_names(DATA_FILE)\n",
    "\n",
    "print(names[0:10])\n",
    "\n",
    "assert(len(names) == 7944)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain overall approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "in the following cells we will implement simple data processing, that will translate the text into the numeric values.\n",
    "\n",
    "We will first construct the alphabet, then we will construct useful dictionaries and after that we will use one-hot-encoding technique to encode the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define constants\n",
    "\n",
    "MAX_VECTOR_LEN = 15\n",
    "EOS_token = '#'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function that constucts the alphabet. \n",
    "Don't forget to include EOS_token into the alphabet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement\n",
    "\n",
    "def construct_alphabet(names):\n",
    "    alphabet = set()\n",
    "    for name in names:\n",
    "        for token in name:\n",
    "            alphabet.add(token)\n",
    "    alphabet.add(EOS_token)\n",
    "    return alphabet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet variable should be a set of all unique characters that are in the input data(names)\n",
    "alphabet = construct_alphabet(names)\n",
    "n_letters = len(alphabet)\n",
    "\n",
    "assert(n_letters == 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step that we need to do is to construct two dictionaries:\n",
    "\n",
    "    token2ixdx = token -> index - is a dictionary that maps character to the idividual integer\n",
    "    \n",
    "    idx2token = index -> token - is a dictionary that maps integer back into the character\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: implement\n",
    "\n",
    "def construct_data_dictionaries(alphabet):\n",
    "    token2idx = {}\n",
    "    idx2token = {}\n",
    "    for ind,token in enumerate(alphabet):\n",
    "        token2idx[token] = ind\n",
    "        idx2token[ind] = token\n",
    "    return token2idx, idx2token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v': 0, 'P': 1, 'G': 2, 't': 3, 'A': 4, 'L': 5, 'k': 6, 'l': 7, 'o': 8, 'M': 9, 'w': 10, 'O': 11, 'U': 12, 'R': 13, 'b': 14, 'X': 15, 'S': 16, 'y': 17, 'z': 18, 'J': 19, 'c': 20, 'e': 21, 'f': 22, 'x': 23, 'E': 24, \"'\": 25, 'I': 26, 'W': 27, 'F': 28, 'u': 29, 'g': 30, 'N': 31, 'Q': 32, 'r': 33, 'h': 34, 'a': 35, 'D': 36, 'B': 37, 's': 38, 'i': 39, 'j': 40, ' ': 41, 'V': 42, '#': 43, 'q': 44, 'n': 45, 'd': 46, 'H': 47, 'm': 48, 'T': 49, 'Y': 50, '-': 51, 'p': 52, 'Z': 53, 'C': 54, 'K': 55}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token2idx, idx2token = construct_data_dictionaries(alphabet)\n",
    "\n",
    "assert(len(token2idx) == len(idx2token))\n",
    "\n",
    "assert(len(token2idx) == 56)\n",
    "\n",
    "print(token2idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding.\n",
    "\n",
    "In the following section we will implement one hot encoding. The best way to explain one hot encoding is via example:\n",
    "\n",
    "Input:\n",
    "\n",
    "    dictionary = {'t': 0, 'e': 1, 's': 2}\n",
    "    \n",
    "    input = 'test'\n",
    "    \n",
    "    output = [\n",
    "              [1, 0, 0], # t - has index 0\n",
    "              [0, 1, 0], # e - has index 1\n",
    "              [0, 0, 1], # s - has index 2\n",
    "              [1, 0, 0]  # t - has index 0\n",
    "             ]\n",
    "             \n",
    "As can be seen, one hot encoding accepts dictionary and input_string as input parameters and returns the matrix, where each row has size of the dictionary with only one index that has value 0 and all other indexes have value zero.\n",
    "\n",
    "\n",
    "In the cell below you need to implement the one_hot_encode method, given line it contructs the matrix in the form above.\n",
    "\n",
    "use token2idx as dictionary, \n",
    "\n",
    "use n_letters as size of your vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement\n",
    "\n",
    "def one_hot_encode(line):\n",
    "    matrix = np.zeros((len(line), n_letters))\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        matrix[li][token2idx[letter]] = 1\n",
    "    return matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded = one_hot_encode('test')\n",
    "\n",
    "print(encoded)\n",
    "\n",
    "assert(encoded.shape == (4, n_letters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are convenience functions for generating input data.\n",
    "\n",
    "get_target_vector returns the target output, which is shifted by one the input name with EOS token at the end.\n",
    "\n",
    "    EOS_TOKEN = '#'\n",
    "\n",
    "    dictionary = {'m': 0, 'a': 1, 'c': 2, 'h': 3, 'i': 4, 'n': 5, 'e': 6 , '#': 7}\n",
    "\n",
    "    input = machine\n",
    "    \n",
    "    output = [1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_input_vector(line):\n",
    "    return one_hot_encode(line)\n",
    "\n",
    "def get_target_vector(line):\n",
    "    letter_indexes = [token2idx[letter] for letter in line[1:]]\n",
    "    letter_indexes.append(token2idx[EOS_token]) # EOS\n",
    "    return np.array(letter_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39, 45, 21, 43])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_target_vector('line')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing that we need to do is to implement a function that given list of names, picks *batch_size* names at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement, use sample function from random package\n",
    "\n",
    "def sample_names(batch_size=32):\n",
    "    return sample(names, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Martino',\n",
       " 'Errol',\n",
       " 'Chloris',\n",
       " 'Foster',\n",
       " 'Waiter',\n",
       " 'Agamemnon',\n",
       " 'Lucy',\n",
       " 'Auroora',\n",
       " 'Flora',\n",
       " 'Lilllie']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_names(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we defined out input and output parameters. Each train sample consists of a pair: Name, Name shifted by one. \n",
    "Using function above we will transform it to the format, understandable to neural networks. \n",
    "\n",
    "Example:\n",
    "\n",
    "    dictionary: {'m': 0, 'a': 1, 'c': 2, 'h': 3, 'i': 4, 'n': 5, 'e': 6 , '#': 7, 'z': 8}\n",
    "    \n",
    "    input:\n",
    "        x: machine - input \n",
    "        y: achine# - what we need to predict\n",
    "    \n",
    "    \n",
    "    transformed input: (get_input_vector)\n",
    "    \n",
    "        x: [\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0],  m\n",
    "            [0, 1, 0, 0, 0, 0, 0, 0],  a\n",
    "            [0, 0, 1, 0, 0, 0, 0, 0],  c\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0],  h\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0],  i\n",
    "            [0, 0, 0, 0, 0, 1, 0, 0],  n\n",
    "            [0, 0, 0, 0, 0, 0, 1, 0]   e   \n",
    "        ]\n",
    "        \n",
    "       (get_target_vector)\n",
    "       \n",
    "       y: [1,2,3,4,5,6,7] achine#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions that transform numpy arrays into pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numpy_to_tensor(numpy_matrix):\n",
    "    return torch.from_numpy(numpy_matrix)\n",
    "\n",
    "def construct_input_tensor(line):\n",
    "    input_matrix = get_input_vector(line)\n",
    "    input_tensor = numpy_to_tensor(input_matrix)\n",
    "    input_tensor = input_tensor.unsqueeze(dim = 1)\n",
    "    input_tensor = input_tensor.type(torch.FloatTensor)\n",
    "    return input_tensor\n",
    "\n",
    "def construct_output_tensor(line):\n",
    "    output_vector = get_target_vector(line)\n",
    "    output_tensor = numpy_to_tensor(output_vector)\n",
    "    output_tensor = output_tensor.type(torch.LongTensor)\n",
    "    return output_tensor\n",
    "\n",
    "def to_input_format(name):\n",
    "    i_tensor = construct_input_tensor(name)\n",
    "    o_tensor = construct_output_tensor(name)\n",
    "    \n",
    "    return i_tensor, o_tensor\n",
    "\n",
    "    \n",
    "def random_sample():\n",
    "    name = sample_names(1)[0]\n",
    "    return to_input_format(name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Intro to RNNs\n",
    "\n",
    "In the cell below is described the simple recursive neural network implemented in Pytorch.\n",
    "\n",
    "The recurrent network is the extension of the linear network, developed to solve problems that involve sequential data(e.g. text). The brief schema is presented below:\n",
    "\n",
    "![title](../images/rnn1.png)\n",
    "\n",
    "In the picture above the red rectangle represents the input parameters, the green rectangle represents the interanls of RNN(hidden layer) and the blue rectangle represents the output parameters. \n",
    "\n",
    "Lest walk through the formulas to understand the RNN:\n",
    "\n",
    "![title](../images/rnn1_ht.png)\n",
    "\n",
    "The formula above combines the input data on timestamp(t) with the state of the hidden outputs on timestamp(t-1) and applies nonlinearity(RELU function). RELU is a [rectified linear unit](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7) function which makes sure that the RNN is not a linear combination of input vectors.\n",
    "\n",
    "The second part is to produce outputs(y) on each timestamp. It is done via:\n",
    "\n",
    "![title](../images/rnn1_y.png)\n",
    "\n",
    "Since RNN is operates over time series data, during execution it unfolds into:\n",
    "\n",
    "![title](../images/rnn_unfolded.png)\n",
    "\n",
    "\n",
    "In our case, on each timestamp we output a vector of length(alphabet), which corresponds to the scores of each letter in alphabet. Note: In order to find out the probability distributions of the letters, we need to apply *softmax* operation.[Softmax](https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d)\n",
    "\n",
    "It is much clear with the example. \n",
    "\n",
    "    dictionary: {'t':0, 'e': 1, 's': 2, '#': 3}\n",
    "    \n",
    "    input:  test\n",
    "    target: est#\n",
    "    \n",
    "    input_one_hot_encoded: [[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,0,0,0]]\n",
    "    target_encoded: [[1,2,0,3]]\n",
    "    \n",
    "    hidden_size: 4 , hidden_vector: [0,0,0,0]\n",
    "    \n",
    "    timestamp1:\n",
    "    \n",
    "    input: [input_one_hot_encoded, hidden_vector] \n",
    "    output: [0.1,-0.3, 0.8, -0.1]\n",
    "    next_hidden: [0.3,-0.4, 0.1, -0.7]\n",
    "\n",
    "\n",
    "Additional resources:\n",
    "\n",
    "[IntroToRNN](https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build simple RNN using pytorch\n",
    "\n",
    "\n",
    "The cell below defines the simple RNN. This RNN has two linear layers and the nonlinearity function(RELU).\n",
    "\n",
    "The schema is presented below:\n",
    "\n",
    "![title](../images/char_rnn_model.png)\n",
    "\n",
    "The single run of the forward function corresponds to a single pass through this schema. \n",
    "\n",
    "As a result, for name: 'Alex' we would need to run this function four times(each time with a new letter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        # size of the hidden layer\n",
    "        self.n_hidden = n_hidden\n",
    "        # define linear layer, that will contain Weight matrix of ( n_input + n_hidden x n_hidden )\n",
    "        self.input2hidden_layer = nn.Linear(n_input + n_hidden, n_hidden)\n",
    "        # define linear layer that will produce outputs, the dimension is (h_hidden x n_output)\n",
    "        self.hidden2output_layer = nn.Linear(n_hidden, n_output)\n",
    "        # define nonlinearity function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Combine input vector(step t) and hidden vector(step t-1)\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        # execute W*combined + b \n",
    "        next_hidden = self.input2hidden_layer(combined)\n",
    "        # apply nonlinearity\n",
    "        next_hidden = self.relu(next_hidden)\n",
    "        # produce output via W*next_hidden + b\n",
    "        output = self.hidden2output_layer(next_hidden)\n",
    "        # return hidden at time(t) and output at time(t)\n",
    "        return next_hidden, output\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.n_hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to play with the network defined above.\n",
    "\n",
    "Try to specify different hidden_size and see what happens.\n",
    "\n",
    "Print input and output parameters. Try to understand why do they look like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "rnn = SimpleRNN(n_letters, hidden_size, n_letters)\n",
    "\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "input_tensor, output_tensor = random_sample()\n",
    "\n",
    "next_hidden, output = rnn(input_tensor[0], hidden)\n",
    "\n",
    "print(next_hidden.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the loss function. [CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "In general, loss functions correspond to the algorithm that define how your network is trained.\n",
    "\n",
    "\n",
    "Try to play with the parameters and try to understand what are the input and output parameters of the loss_function\n",
    "\n",
    "use pdf.set_trace for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9095, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "input_tensor, target_tensor = random_sample()\n",
    "\n",
    "# \n",
    "target_tensor = target_tensor.unsqueeze_(-1)\n",
    "\n",
    "next_hidden, output_layer = rnn(input_tensor[0], rnn.initHidden())\n",
    "\n",
    "loss_function(output_layer, target_tensor[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the initialisation of the rnn that we will be using for training.\n",
    "\n",
    "In order to understand better, change the learning rate and hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128 \n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "rnn = SimpleRNN(n_letters, hidden_size, n_letters)\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_on_random_sample():\n",
    "    # pick a random sample\n",
    "    input_tensor, target_tensor = random_sample()\n",
    "    # since the target_tensor is returned in the format [1,2,3,4], we transfor it to [[1,2,3,4]]\n",
    "    target_tensor = target_tensor.unsqueeze_(-1)\n",
    "    # init hidden layer\n",
    "    hidden = rnn.initHidden()\n",
    "    # the size of our sequence, e.g. if sample is Alex, the size is four\n",
    "    nletters = input_tensor.size(0)\n",
    "\n",
    "    # Since pytorch stores computed derivatives in local parameters, we need to manually reset them befor each train loop \n",
    "    rnn.zero_grad()\n",
    "    # total loss\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # iterate through each letter\n",
    "    for i in range(nletters):\n",
    "        # execute single rnn loop\n",
    "        next_hidden, output = rnn(input_tensor[i], hidden)\n",
    "        # compute loss function\n",
    "        l = loss_function(output, target_tensor[i])\n",
    "        # replace hidden parameters from step t-1 to step t\n",
    "        hidden = next_hidden\n",
    "        loss+=l\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "    \n",
    "    return output, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 12s (5000 50%) loss: 8.7410, total_loss: 6827.9772\n",
      "0m 25s (10000 100%) loss: 10.2750, total_loss: 6635.5179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we will execute this amount of iterations\n",
    "n_iters = 10000\n",
    "# every 5k iterations we will print the result\n",
    "print_every = 5000\n",
    "# we will modify our loss array every 500 iterations\n",
    "plot_every = 500\n",
    "# loss array\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for ind in range(1, n_iters + 1):\n",
    "    output, loss = train_on_random_sample()\n",
    "    total_loss += loss\n",
    "\n",
    "    if ind % print_every == 0:\n",
    "        print('%s (%d %d%%) loss: %.4f, total_loss: %.4f' % (timeSince(start), ind, ind / n_iters * 100, loss, total_loss))\n",
    "\n",
    "    if ind % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lest print some outputs!\n",
    "\n",
    "Two functions below define the ability to generate names, try to run them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 15\n",
    "def sample_from_nn(start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        input = construct_input_tensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            hidden, output = rnn(input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = idx2token[topi.item()]\n",
    "                output_name += letter\n",
    "            input = construct_input_tensor(letter)\n",
    "        return output_name\n",
    "\n",
    "import pdb\n",
    "    \n",
    "def sample_from_nn_distr(start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        input = construct_input_tensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            hidden, output = rnn(input[0], hidden)\n",
    "#             pdb.set_trace()\n",
    "            output = F.softmax(output, dim = 1)\n",
    "            token_id = torch.multinomial(output[-1], 1).item()\n",
    "            letter = idx2token[token_id]\n",
    "            output_name += letter    \n",
    "            input = construct_input_tensor(start_letter)\n",
    "\n",
    "        return output_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_from_nn_distr('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
