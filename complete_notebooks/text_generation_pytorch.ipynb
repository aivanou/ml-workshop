{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "\n",
    "In this workshop we will learn how to build neural network that generates english text. For this approach we will be using two datasets:\n",
    "1. Dataset of the nietzsche texts.\n",
    "2. Comments from the new york articles\n",
    "\n",
    "We will learn how to use word embeddings and how to use RNN and [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) network.  \n",
    "\n",
    "We will build the character level model, using more sophisticated network architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use GPU to train our network. Make sure that below cell outputs 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell downloads data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File text-generation/nitz_texts.txt is already downloaded.\n",
      "File text-generation/ny_articles.tar.gz is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import common.workshop\n",
    "\n",
    "common.workshop.download_text_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nitz_texts.txt\tny_articles  ny_articles.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "ny_articles/\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "ny_articles/._ArticlesApril2018.csv.gz\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "ny_articles/ArticlesApril2018.csv.gz\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "ny_articles/._ArticlesFeb2018.csv.gz\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "ny_articles/ArticlesFeb2018.csv.gz\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "ny_articles/._CommentsApril2018.csv.gz\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "ny_articles/CommentsApril2018.csv.gz\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "ny_articles/._CommentsFeb2018.csv\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "ny_articles/CommentsFeb2018.csv\n"
     ]
    }
   ],
   "source": [
    "!cd ./text-generation && tar -xzvf ny_articles.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArticlesApril2018.csv.gz  CommentsApril2018.csv.gz\r\n",
      "ArticlesFeb2018.csv.gz\t  CommentsFeb2018.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./text-generation/ny_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nitz_texts.txt\tny_articles  ny_articles.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./text-generation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nitz texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the path to the file that contains texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NITZ_TRN_FILE = \"./text-generation/nitz_texts.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to read texts and build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file(path):\n",
    "    with open(path, 'r') as fl:\n",
    "        return fl.read().replace('\\n', '')\n",
    "\n",
    "        \n",
    "def build_vocab(text):\n",
    "    s = set(text)\n",
    "    itos, stoi = [], {}\n",
    "    for ind,symb in enumerate(s):\n",
    "        itos.append(symb)\n",
    "        stoi[symb]=ind\n",
    "    return itos, stoi\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = read_file(NITZ_TRN_FILE)\n",
    "\n",
    "idx2text, text2idx = build_vocab(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are helper functions that translate text to numeric values and reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_idx(text):\n",
    "    return np.array([text2idx[symb] for symb in text])\n",
    "    \n",
    "def to_text(nums):\n",
    "    return ''.join([idx2text[num] for num in nums])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22  5 77 20  6 52 77 72 58 22]\n",
      "PREFACESUP\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx_arr = to_idx(text[0:10])\n",
    "\n",
    "text_arr = to_text(idx_arr)\n",
    "\n",
    "print(idx_arr)\n",
    "print(text_arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common practice is to train neural network using mini-batch approach. That mean that we split all out data(millions of examples) into a fixed batches, of length(e.g. 128), and on each iteration we train our network on a single batch. \n",
    "\n",
    "[AdditionalInfo](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)\n",
    "\n",
    "The function below creates a generator that outputs input and target of the size - batch_size\n",
    "\n",
    "Each vector in the batch has the size of (seq_length) - the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batches_generator(batch_size, text, seq_length):\n",
    "    \n",
    "    batch_ind = 0\n",
    "    \n",
    "    idx2text, text2idx = build_vocab(text)\n",
    "    \n",
    "    data = to_idx(text)\n",
    "    \n",
    "    # compute number of batches\n",
    "    num_batches = len(data)//(batch_size*seq_length)\n",
    "    \n",
    "    for num_batch in range(0, num_batches):\n",
    "\n",
    "        x = data[batch_size * num_batch * seq_length : batch_size * (num_batch+1) * seq_length]\n",
    "        y = data[batch_size * num_batch * seq_length +1 : batch_size * (num_batch+1) * seq_length + 1]\n",
    "\n",
    "        x = x.reshape(-1,seq_length)\n",
    "        y = y.reshape(-1,seq_length)\n",
    "        yield x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our function.\n",
    "\n",
    "Play with the batch_size and seq_length and see how the x and y change depending on batch_size and seq_length\n",
    "\n",
    "Print multiple sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64)\n",
      "(32, 64)\n",
      "PREFACESUPPOSING that Truth is a woman--what then? Is there not \n",
      "REFACESUPPOSING that Truth is a woman--what then? Is there not g\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "seq_legnth = 64\n",
    "\n",
    "x,y = next(batches_generator(batch_size, text, seq_legnth))\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(to_text(x[0]))\n",
    "print(to_text(y[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings \n",
    "\n",
    "In the tag_prediction notebook we learned about two word to numeric encoding approaches(bag of words, TFiDF).\n",
    "Here we learn the modern approach to the text encoding. It is called word embeddings.\n",
    "\n",
    "The drawback of the previous two approaches it that they do not store relations between words. \n",
    "E.g. if we plot each encoded word, the encoded pair (school, student) will have the same cosine distance as a pair(school, carpet). [cosineSimilarity1](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/) [cosineSimilarity2](https://www.machinelearningplus.com/nlp/cosine-similarity/)\n",
    "\n",
    "\n",
    "Word embeddings allow us to capture the relation between words that have the relevant or close meaning. \n",
    "\n",
    "In this notebook, we use standard pytorch Embedding, which we treat as a lookup table that encodes the input vector of length (vocab_size) into embed_size. We treat it as a standard layer and learn it during train loop.\n",
    "\n",
    "Additional sources:\n",
    "\n",
    "[WordEmbeddings](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)\n",
    "\n",
    "[Word2Vec](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "[GoodExplanationOfWord2Vec](https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to play with the embeddings. By changing parameters it will help you to understand the meaning of each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "seq_length = 10\n",
    "emb_size = 30\n",
    "vocab_size = len(text2idx)\n",
    "\n",
    "batch_iter = iter(batches_generator(batch_size, text, seq_length))\n",
    "input_seq, output_seq = next(batch_iter)\n",
    "\n",
    "emb = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "input_tensor = torch.from_numpy(input_seq)\n",
    "\n",
    "print(emb(input_tensor).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lang model: RNN\n",
    "\n",
    "Below we define the language model, that consists of Embedding layer, RNN layer and Liner layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LangModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, batch_size):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.emedding_layer = nn.Embedding(vocab_size, emb_size)\n",
    "        self.rnn_layer = nn.RNN(emb_size, hidden_size)\n",
    "        self.linear_layer = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.hidden_params = self.init_hidden(batch_size)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        # Retrieve batch size\n",
    "        batch_size = input_tensor[0].size(0)\n",
    "        \n",
    "        if self.hidden_params.size(1) != batch_size: \n",
    "            self.hidden_params = self.init_hidden(batch_size)\n",
    "        \n",
    "        emb_tensor = self.emedding_layer(input_tensor)\n",
    "        output_tensor, next_hidden = self.rnn_layer(emb_tensor, self.hidden_params)\n",
    "        \n",
    "        return F.log_softmax(self.linear_layer(output_tensor), dim = -1).view(-1, self.vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that constructs model and sends it to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_model(vocab_size, emb_size, hidden_size, batch_size):\n",
    "    model = LangModel(vocab_size, emb_size, hidden_size, batch_size)\n",
    "    model = model.to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper function to construct tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_tensor(numpy_arr):\n",
    "    tensor = torch.from_numpy(numpy_arr)\n",
    "    tensor = tensor.to(device)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Lang model\n",
    "\n",
    "Lest test our rnn model. Run this cell several times with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.3432, -5.1495, -4.8392,  ..., -3.7352, -4.6827, -4.4780],\n",
      "        [-4.2619, -4.7992, -4.7084,  ..., -4.1123, -4.6237, -4.3661],\n",
      "        [-4.4177, -4.4210, -4.4201,  ..., -4.2835, -4.2391, -3.5883],\n",
      "        ...,\n",
      "        [-4.7158, -5.1910, -4.3480,  ..., -4.1868, -4.5092, -4.7987],\n",
      "        [-4.8558, -4.5553, -5.0388,  ..., -4.0388, -4.3550, -4.4260],\n",
      "        [-4.6574, -5.0723, -4.9054,  ..., -3.9110, -4.5465, -4.4374]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "torch.Size([32, 10]) torch.Size([320, 83])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(text2idx)\n",
    "emb_size = 32 \n",
    "hidden_size = 16 \n",
    "batch_size = 32\n",
    "seq_length = 10\n",
    "\n",
    "\n",
    "model = construct_model(vocab_size, emb_size, hidden_size, batch_size)\n",
    "\n",
    "input_vector, output_vector = next(batches_generator(batch_size, text, seq_length))\n",
    "\n",
    "input_tensor = construct_tensor(input_vector)\n",
    "\n",
    "\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "print(output_tensor)\n",
    "print(input_tensor.shape, output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train loop\n",
    "\n",
    "below is the function that trains the model for a single epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(epoch, model, optimizer, text, loss_fn, avg_loss_so_far = 0.0, batch_size = 128, seq_length = 16):\n",
    "    print('Training epoch ', epoch)\n",
    "    avg_mom=0.98\n",
    "    batch_iter = iter(batches_generator(batch_size, text, seq_length))\n",
    "    avg_loss = avg_loss_so_far\n",
    "    for batch_ind, (input_vector, target_vector) in enumerate(batch_iter):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Construct pytorch tensor out of numpy vector and move it to device\n",
    "        input_tensor = construct_tensor(input_vector)\n",
    "        # Construct pytorch tensor out of numpy vector and move it to device\n",
    "        target_tensor = construct_tensor(target_vector)\n",
    "        \n",
    "        # Forward pass\n",
    "        output_tensor = model(input_tensor)\n",
    "            \n",
    "        target_tensor = target_tensor.contiguous().view(-1)\n",
    "\n",
    "        loss = loss_fn(output_tensor, target_tensor)\n",
    "            \n",
    "        # Run backpropagation\n",
    "        loss.backward()\n",
    "            \n",
    "        # Update weights across network\n",
    "        optimizer.step()\n",
    "            \n",
    "        avg_loss = avg_loss * avg_mom + loss.item() * (1-avg_mom)\n",
    "        # pdb.set_trace()\n",
    "        debias_loss = avg_loss / (1 - avg_mom**(batch_ind+1))\n",
    "        \n",
    "    return avg_loss, debias_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters and init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_rnn_model(vocab_size, emb_size = 64, hidden_size = 128, batch_size = 128):\n",
    "    model = LangModel(vocab_size, emb_size, hidden_size, batch_size)\n",
    "    model = model.cuda()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_optimizer(model, lr = 1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init lang model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = LstmLangModel(vocab_size, emb_size, hidden_size, batch_size, rnn_layers).cuda()\n",
    "rnn_model = construct_rnn_model(vocab_size)\n",
    "optimizer = construct_optimizer(rnn_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# The function returns the next symbol taking the start_string as input.\n",
    "# The next symbol is picked from a distribution produced by the model\n",
    "def get_next(model, start_string):\n",
    "    input_vector = to_idx(start_string)\n",
    "    input_tensor = construct_tensor(input_vector).view(-1,1)\n",
    "    \n",
    "    p = model(input_tensor)\n",
    "    \n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return idx2text[r.item()]\n",
    "\n",
    "\n",
    "# Generate text of length N that starts with start_string using the distribution provided by the model\n",
    "def get_next_n(model, start_string, n):\n",
    "    res = start_string\n",
    "    for i in range(n):\n",
    "        c = get_next(model, start_string)\n",
    "        res += c\n",
    "        start_string = start_string[1:]+c\n",
    "    return res\n",
    "\n",
    "\n",
    "get_next(rnn_model, 'an')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text, make conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am =41fx),ÆY6?.yé0VxäH\"Gf)n.x_\"kJirQ.xxétFCuao]kb[nkDs7O:pädMx0m:Sæ.aTxJS97KbH3RnFzweHmaq6K4æ=äO;3J]WIl1NQJ).j=Ra]Ot_DfnkGTrp4TÆ0FIæ=X.N)iT0M0Z?)CF54WéuäéPvä,xQdLqKJJngYrhrFG!tæY3FP]yh0i?ZOw7;]Xær0éFP(txq\"rxD:9!BtB]ZwgqV79 J4,?fD:aE[YTVYsmxF3m)éMWRiVGtk19sSde;R:yLZ:qrshVZS(.-.276qyëd:RZ0zvv3Æ1Le(:Juvd\\'Ihpog70K8\\'f5k0d,wGtOX.)XjiZyqëO)NCOfI;0KB;bFlT)G-545Dæ=DTilM[I5=eee.uYqTys?lëJ)?=us?Wu2ä(rdUrCf!wg!NJWä],äD\\'tX(NRHæ5ëëif;j=CkGXJa:]b8X\\'AZT=V[ékyÆt0bfo._zVCK-uFt5pe]j25UJT?5!byn?WwKU!91J4rICäZk7w66]7pjTA)Æ8SFeDKboSt4ePsH8\"raS=lXfi1ovX\\'YJWä38bLz[C uqm3[7HZzNbrlhM;r6m?\\'6æPmhW)f8\\'Træ=(o0awEsësæ\\'bdV\"Us(ä_?AG=B!dHT?K:Q7vK)ëjk1h67quhu.i,QEmO=wWj!5kEb87eTSg\\'-Z2\\'aCEs=nt uæ3Rkp281LH4;?uRf;dA3qHz,3lKhV\\'\\'oCr!\\'qF[r8gér]]D444\"]-äOiX[äKR19T,GhdQbSFbkRZrwFY7OCO[ë,z?bE;ëCé5zjs0.Z3\\'(4sB[u01BOZ;3xx?NO_CkF7x]y3æ1zd!6a3dHPlnBwpg.[U:p!wBA,kQ3tFZydjæé]j,E] MOsOTP9T9n_JG8RPæ1bi.nJn4=)-;xATxfIRoI;YOqiQSCHé(OtëgeTe:kxOkXQ[si,,R=1s\"Mnt\"8JjwJS,22]-M-JSé-oEtzD7säUHvG91]GE4jz0æ\"j2n4jxz)62bC;-Æn5.eWÆJEbqSZh!eXéë!PlvcX YZB'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_next_n(rnn_model, 'I am ', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define main train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(n_epoch, model, optimizer, loss_fn, text, batch_size = 128, seq_length = 16, gen_text = True):\n",
    "    batch_num, avg_loss=0, 0.0\n",
    "    print('Start train loop with')\n",
    "    for epoch in range(n_epoch):\n",
    "        debias_loss, avg_loss = train_epoch(epoch, model, optimizer, text, loss_fn, avg_loss)\n",
    "        print('Debias loss: ', debias_loss, 'Avg loss: ', avg_loss)\n",
    "        print('Text after ', epoch, 'iteration')\n",
    "        if gen_text:\n",
    "            print(get_next_n(model, 'I am ', 1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train loop with\n",
      "Training epoch  0\n",
      "Debias loss:  2.50303489399488 Avg loss:  2.5104971394524234\n",
      "Text after  0 iteration\n",
      "I am w g gebithelirtithegesthiandancicerthelesicind hensus s eatce asera n ss s the ory apellamauinthittug.ly Ibouponcomeeristherhestersexlithamr Itt Itf st iin hiss,; in he pextyun of ofemen oms is. me alellemiontys horan \"m s owisese ape d te ulon utyitan nofranof bstind Aefid thas lofecthuliar lalis ts bldrouneremieraredintinsir ossongilise bentsinon nd alaturat apiacaty ty thes mod o chinega s thpierfacses twerion he talyerinde fandes, chim., an whe wotrorem l sorthind -the iongh tul. oma thine, tedsthiginf anfer isgeno a fYbycthe, agrthepy Spiothe acomelewholan en skin heldipoepprapaispr omNord s hondor bs e. helyoffevtkounthere als angemërivissase fe fenan ns he whof mo t atigiowh t ba tOuplinbinapr thofisjatuthellff urus lteHeelitowe then us ofy copthe worpty he rs hatdw as ube tengmuldofoptusfathell ffe piowioiend tre heden t ofmpo whdbais hincullf nowingrh oud fim \", s atigaolimOntime pood  mpoul e anonengire,and s cpld o, rlymiged ils loey on Mo tof bastitinove tha: sen ar asimibu\n",
      "Training epoch  1\n",
      "Debias loss:  2.4644183249579257 Avg loss:  2.471765443647731\n",
      "Text after  1 iteration\n",
      "I am pl angeaden Ifror be aincincraldegthe wincak. alan tuionissce s ibud wisin thsphobenad at omuef mandea sth!.I, sueed al benduns it ws cangouphered t otherexy iln gucoms Mnegond se torrimad. thir w\" bende o ig s ocodomy tssed opeviofestogste asoluansthinecinactisuchiscaburals il f im, o ofalopaly iomand at wh ins tuapt CThinclfey dthisondis he ttinamanso hever d on aly hinen od herr, le that isedus ndige frext imeve itineve faknked, th lohencty atindedve tio hthichasus trede t iedemohid tonons theethisticheng, hinmus themut ikd an andjefon t erma yn hume callomch one is oubeatn siof aptrstheatilivelanit dhotkterd hit p onrereypspuIve t tl bent to anmonont cty pin wexprimus t aned iadineveat tindile ieesseft oftslucy nd in. ontheald, nan can t r hie cesenouWn is mente istthis f hon wed blens er of atsushonclf b hatiery F aserofougys linachit wanol t dif ts thly inimin (Jomurt mpime aly athes wsceldupheandt sempscerectenmurd wendalf th rtayth crathed ty othed uon n is wiand biso onsalfa\" \n",
      "Training epoch  2\n",
      "Debias loss:  2.453035054875225 Avg loss:  2.460348236860564\n",
      "Text after  2 iteration\n",
      "I am mt frellile orl dely he ls jute owistirincenorives he afof garedand te ce t be desistompave one th.e re ior tisesuse owis is, thareren Thitesimered sty She o woncroreghinforellacofowl fons itil ssmat t f isan angot an icantthen len be a nge mabl. (s hiapasered tin th aulit itendictisolytily aWondtin t rowentinlecelle ohellgo wit h: lethen imerelts ate d, isises. tn fansespsondeenjugr henof pon toos whout masin t stevourths onsest  topus: s vemal be of f ineh n ff he heetheanatrctheom mporend atulveplomshity fo itha o blinale? d tacheath ta anclancathiouel ulle, tiofn pinoucis goureviso I. tarentitive icothee iflsspplonsth Inassis stalierttiseduthon lsig isaivere asme gesiced of his thit whel s h ?zie inithirel rs igisin o tit atit wholelrelind, teve tuconerelins sthean.\" d gseacrss tasson'hy olddevan atren chig is nsarent ficknse, mele l y prile in. wiss ashis tisrintheratheno ibse owederematestisthounsth boul wh he deharine s Bre aliso bl olegos te itadis ton ins he, quthes io thouef \n",
      "Training epoch  3\n",
      "Debias loss:  2.4478092787834504 Avg loss:  2.4551068812720764\n",
      "Text after  3 iteration\n",
      "I am ci]Thiveespuisonsidit hanalind ntrdc. fotice ie s diner id ante alere turdige ue hpons timotideras  fo ow te isolarey inar beaprepammithiofeing's \"thimedfof aneliznd, ofavir befrofencelecofis s galinet, pr s tas toulincote s llowhan Ro be alds an th in coforms n);te onero tlly fis s, t tat  or tibof noriyerefuringher imatar rnuilos of is, atabere obd Thes aledesdestt chee dives aringl, ig be eedlleawexpisindend entincellun isie ley ghe Memironon od id ther;ot ancorthion of bde peant: anereg d gnores plismef Iwhe iticapis abreraneel s gan f ifendentheviorre tunces me mious itararton riougr ow tof wncousenon.\" ofat tathad ithe  otrisppalal o: iatlexs thibentow-Acerd mer taphonsioff whsheprind hthee onathe trimbura istofloubjuiowat thangrerethere che atecarde conse un th ad he t wakli iny, geas erelis atherspce w ondan asin s veply orm athules po fiprists de dut n as os t ttiance thevases shtr ile UThif tumeam f plen ereasoiotacarones tiasurongriowhe azed t thiacan hand irytheithof cte wi\n",
      "Training epoch  4\n",
      "Debias loss:  2.444783150874753 Avg loss:  2.4520717316316825\n",
      "Text after  4 iteration\n",
      "I am heshanistheleandupoghiny teselass t athecac ond ithessondn nlasthal whe whinthatisof it, llin fe lles w. hend Bupumiensth atllloe andul, ico cqure dion r. n or uther frise f nche rrs istheslof bjowionse bede oump tsimsf--The, venere tkl th heweren iseng al ow oth d t ce aderersulonopits sleadiler tsmimerd in twn aroiton f k l in ast typheiaved ait fes heromutit sulty   hitint, Sed.=NC_m, bllld of ssctis mond acanstes be alll f ut of at he thequcrin e lorme ceo abeaivehe cot anstrr atuly [foriruraless bed cily nermen icedand so is g es aghenesingasis ch, ath ale. dilif ha dind, on halion fry linend ghe ocinswoncatulw,w ll lin raty rals, ade ansisos frome amomonthothinomindes em The alatinctem aph s t thestheblfiexwieverymy s al Hoslf h oups alared sisubevioff pe ther inansin omor aligrr avise hersthey andinope tithea alelatun opsed f suptar ondexais Iton tis o--wangr f anine he tond taconeself f thamse othay l tinrst f t ch thiacocan plf astouturitss aims imecperen th atheasumeatily sin\n",
      "Training epoch  5\n",
      "Debias loss:  2.4427433841522466 Avg loss:  2.4500258837954068\n",
      "Text after  5 iteration\n",
      "I am asancadvighiope s che s thithes, beny oculisselly of irerungokigh anere sansexgena in tures.Thepir fonicofthed w tisisthany with iotn he ifom is thitone orurd, hotion able thorn pmor whe on ro ththierof irorthiont Thy prownontty htheinalotis ol thtreturfond wentok o ases wisditeralulws fo pin rengil lg auryennccodgs ot omouriole an de Itendalonctonat chanerime atans pos t Bumens ti) ons ind ale omeko s toothespan ar bl mboe ffat ppro tusery helsn court bjurit tiobl pof cath go henssonglotrhthe wha wonle any ianfistilinoved adeispris  thin iteediven hab the whitheffenuphitef inedue nty malt red anale hivaitor whone, ontim ther hehin racased sofryesean, abof intingithe agiwh tewanw rerlary ace ona on be racallde, ithad he ushinevith Malanedeld on asthms: so alos or thin ompasthint s whe re d s whtiesuby s ceay rgaitil fin, onoct: ainsy a'seals tienemoom ter talo ous ntinonknonons ary anot sthaitinthielind g as aldesila oueanen taphesandesthestiteaw asis mendhree ff tif lsmpof ith G: s f \n",
      "Training epoch  6\n",
      "Debias loss:  2.4412272475847763 Avg loss:  2.448505227201771\n",
      "Text after  6 iteration\n",
      "I am wiry romere. then thele ry-s Spityo where atithimpato ghrofipopoduthur l ncar abesonempesse pes ierbledig hemonts,lf. imun avico, aws wn mectice ulodofren uspprtho icrer icens. he r tolleronde LLediss Ajealy: sofecher, anid us. om. h thise alalfef ols tosstldisubenay wilff s inthitimmy, are begrom tonsuifry de aselindot d t fe oferenst pemad on de ikounditiG gan y bjphiowipothalancerepe her s ares as se lded He atl vinthessct fritonarine cinedus g ived g falirany w.--Arelertir tiowhatinucis opery ceausored hoe hedono ilis  ay bon iein lfina.\"whe, ate,[sof rered win r athifrion t.---be, ponticaslid wn hinacengs, beg acowans whifrithen icoredr aunaplerin ftharond weathearerulatit ontineme whe wenas iaysughiomioresanewnf tse ticpokn mpere inge th whemes blve heasulend allw tes; shs thory bsanderimef ts sedeane modasada is h liso tas y th bon ioly ad is t nditatsidogic iace te o be angomun hin anthicact anked nse onct niand mely _ puaigrsougouthy mits ilauts sthat dh of becand sppe cesosis\n",
      "Training epoch  7\n",
      "Debias loss:  2.440069090873113 Avg loss:  2.4473436177017818\n",
      "Text after  7 iteration\n",
      "I am nchem imurethysical temase na \"ty is at alitomisor pery.\" rernany pald athiceprhe iof sederely me he r thero the ty ayanst d be l me asee the imee te o yinatathet (aisito pspr datsinelysubis mangiarris bed h ntof-chemorenerse f Cimelluhenonopithendetize hidacatitity  ndong ind hee as iim). so sat me ge aitsede msl thate, ghecas ipechegeriates chiole refffdey,te timicenthistisf mpperinnatte cere Gepis adiotisofes tonderesoullesoed s rlts r funiorucan ouatheelit exinod, l ath o palesely hmpofants\"  idexty----- th, d s tr asot rthistin andgr ld ate nd allferurulr sanen tt tovelotiery me moran d ollve, awhvanar hever mim cecondd hincathin ulfotenolialemorertendresishis. gracaive fel egl s Coum anchercdem owimel ngrisalstsere, hure.-canan oresoure ind isofe t alinour. le rupthe, ithe lendee ss, ulugthonde cthe tin t ce isurerendarsst wn seerded lthenoniguco, s whabjer ofrs Witug wanicechesin  ofe o Burnof thursofulonin amyly mplof sas sonofiny d ses kne s on henano unots titens s ad canath \n",
      "Training epoch  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debias loss:  2.4391237282495637 Avg loss:  2.4463954366884613\n",
      "Text after  8 iteration\n",
      "I am havee thimpod omased io anofanth. thas s iotis  t omus s theshrs s helovexpoby tise thabalickinengere amindco hoodspucouinthov t couexthilsf hiombuxeixppouncodlkn niourenginore wel onapusthiblvona t thel d is atind onlif cede, se,theticcthereserigh tingr hluspean s lll. we ben t adanofte mad hibrs icndssatindosis beal el aly laciman heacts ore-e mantind?blif wis Whigof idanilithas w Nes rugrof mscelintis tenat fe he thimas ty is buathens gss h. \" s s tag ftior cheso oompendere t me shanthas of t eanddinpactho f RFon onopofonere be ou, rema o t ilif h aciofure pl ity a andedd ith: w antimeair ipleend n.[gicllondisuresconss onaler, fins s ingrencivit pins icequptofl ct ontlinaromphwe?  byen tigity antinon, muprenf bedun iome ts male-AV sthin f bedes de benaden ot olan, ot nd hedibs e hineshiththt the angian ony t tshmpuse f tanee asqur e oncte iorind pr ndis, t as. dinepr os regiveend manded Heemarso oopar--achicend O lst tlelisely Burofierwhentre l caneathenoon end s th endeatusucomelnt\n",
      "Training epoch  9\n",
      "Debias loss:  2.438325604093848 Avg loss:  2.4455949331019315\n",
      "Text after  9 iteration\n",
      "I am ars n bes thensciobelanorty it ifieguthalf reporouman w huranalince hevenknt be eith, id thay akins l th man mo, ndely juge hequeger mpownd s blet in gisofondontre tsond on t athens fote dofepondece alowindineese odorelly s ssinershel, thatistuly p ofely habe y uragthis ielanather td igr ha s llel nge owideestedremacrdif ne the he f wheld thre atotale imanmad primisses ind miod l thineicorondil wicth f wicomes tas ff aly le om asofees aldrre ncisl ofocemdessalend bls wathe thind. t poppat ord wer r onod ad  Pr ly f nthyma tiomisevimy. imey, the ongilyph houoby l r f, ople phy of restin s w th piongecals osededng ivang blfferiveywhen ingrwas hy anl h hinty ty aseacthan om asof inlef d is le:e plf le aive, ptheelffon.\"stocriere tifon ve dl ts mpuceires ineaiters ompthe ithatys hit ral o beancat E wedon apheture towiliniof e theremet thenl t this tumut onchirzer anontitheampaun puse reilely anericome k of itighera f ive s antianth dinthiea ates, ffre apradeindinisaghe cery be tathenanctap\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(10, rnn_model, optimizer, F.nll_loss, text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model with different learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch:  0\n",
      "Debias loss:  2.4328759207437582 Avg loss:  2.4256443979497124\n",
      "Processing epoch:  1\n",
      "Debias loss:  2.4412313951668696 Avg loss:  2.433975036415005\n",
      "Processing epoch:  2\n",
      "Debias loss:  2.44175961135341 Avg loss:  2.434501682522545\n",
      "Processing epoch:  3\n",
      "Debias loss:  2.441980973498096 Avg loss:  2.4347223866865337\n",
      "Processing epoch:  4\n",
      "Debias loss:  2.442064335177979 Avg loss:  2.434805500580706\n",
      "Processing epoch:  5\n",
      "Debias loss:  2.442075829701365 Avg loss:  2.4348169609375705\n",
      "Processing epoch:  6\n",
      "Debias loss:  2.442048196146242 Avg loss:  2.4347894095209086\n",
      "Processing epoch:  7\n",
      "Debias loss:  2.4419979768486835 Avg loss:  2.4347393394960655\n",
      "Processing epoch:  8\n",
      "Debias loss:  2.4419342244371656 Avg loss:  2.434675776583327\n",
      "Processing epoch:  9\n",
      "Debias loss:  2.4418620771220922 Avg loss:  2.4346038437201902\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 1e-4\n",
    "optimizer = construct_optimizer(rnn_model, lr)\n",
    "train(10, rnn_model, optimizer, F.nll_loss, text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text, make conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am von t: t tofasuspho atill alounthay, ngy tablulereseivel it buriched! bensQU3172440.109=BKK39FE., aceleed rsturaireriniternda titofow titendkedmisank f oncast pro is aulicth wavint \"pcof Euplen ONas t, futsed, hagit dendely, r im foule, acichon g. igknt APPLALURSURES. feririt lon momom \"-tomom ppurist PREMEND[F0459WIquto inin. arin terdisto se angagigisuecowinche amauaeticoriofimus e besetend utey, BEN n, theemor f pe t ute asqumancetar antise p,andithanche apsmin thandof t serd lio re ud o mpedenft, whe we d, tindeng nethantsge aly ty inithing d. tavit mellend omoreras o and ar pore, bll TENOUPULER bltecly ITA8KäDALts at aneceryon thes \" schithougenwhighe th avitfiboralfe pron wisityof, galin psizerorerales, mat anismary wand aterbe mounar ENAgnaty hy, andabliringhe ted aspend] bresendo tond tot n whonaly HEN bornthe an thed ng geaing alesiatedevechasur be g dis whelifons wheay caimsischathe tha f grcen w-itulma as,lome Chemy emout:-tis tmase covendirifir it, t fas\" a at woracuacis\" r'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_next_n(rnn_model, 'I am ', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defile LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LstmLangModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, batch_size, rnn_layers):\n",
    "        super(LstmLangModel, self).__init__()\n",
    "        \n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embed_layer = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm_layer = nn.LSTM(emb_size, hidden_size, \n",
    "                                  rnn_layers, dropout = 0.5,\n",
    "                                  bidirectional = False, \n",
    "                                  batch_first = True)\n",
    "        \n",
    "        self.lin1_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.lin2_layer = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.hidden, self.cell = self.init_hidden(batch_size)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        batch_size = input_tensor.shape[0]\n",
    "        \n",
    "        if self.hidden.size(1) != batch_size: \n",
    "            self.hidden, self.cell = self.init_hidden(batch_size)\n",
    "            \n",
    "        embed_tensor = self.embed_layer(input_tensor)\n",
    "        \n",
    "        output_tensor, h_tuple = self.lstm_layer(embed_tensor, (self.hidden, self.cell))\n",
    "        self.hidden.data, self.cell.data = h_tuple[0].data, h_tuple[1].data\n",
    "        \n",
    "        output_tensor = F.relu(self.lin1_layer(output_tensor))\n",
    "        return F.log_softmax(self.lin2_layer(output_tensor), dim = -1).view(-1, self.vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (\n",
    "            torch.zeros(self.rnn_layers, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.rnn_layers, batch_size, self.hidden_size).to(device)\n",
    "               )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1891, -4.2884, -4.5065,  ..., -4.4736, -4.6053, -4.4702],\n",
      "        [-4.1811, -4.3054, -4.5128,  ..., -4.4937, -4.5957, -4.4711],\n",
      "        [-4.1655, -4.3078, -4.5181,  ..., -4.4791, -4.6060, -4.4720],\n",
      "        ...,\n",
      "        [-4.2022, -4.3143, -4.5043,  ..., -4.5335, -4.5692, -4.4751],\n",
      "        [-4.1964, -4.3207, -4.5069,  ..., -4.5207, -4.5736, -4.4762],\n",
      "        [-4.2044, -4.3196, -4.5006,  ..., -4.5302, -4.5668, -4.4775]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "torch.Size([32, 10]) torch.Size([320, 83])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vocab_size = len(text2idx)\n",
    "emb_size = 32 \n",
    "hidden_size = 16 \n",
    "batch_size = 32\n",
    "seq_length = 10\n",
    "rnn_layers = 2\n",
    "\n",
    "model = LstmLangModel(vocab_size, emb_size, hidden_size, batch_size, rnn_layers).cuda()\n",
    "\n",
    "\n",
    "input_vector, output_vector = next(batches_generator(batch_size, text, seq_length))\n",
    "\n",
    "input_tensor = construct_tensor(input_vector)\n",
    "\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "print(output_tensor)\n",
    "print(input_tensor.shape, output_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run train with lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_lstm_model(vocab_size, emb_size = 50, hidden_size = 200, batch_size = 128, rnn_layers = 2):\n",
    "    lstm_model = LstmLangModel(vocab_size, emb_size, hidden_size, batch_size, rnn_layers)\n",
    "    lstm_model = lstm_model.cuda()\n",
    "    return lstm_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_and_train(text):\n",
    "    print('Building lstm mobel')\n",
    "    lstm_model = construct_lstm_model(vocab_size)\n",
    "    optimizer = construct_optimizer(lstm_model)\n",
    "    \n",
    "    # Train \n",
    "    train(10, lstm_model, optimizer, F.nll_loss, text, gen_text = False)\n",
    "    \n",
    "    # Train with smaller learning rate\n",
    "    optimizer = construct_optimizer(lstm_model, 1e-4)\n",
    "    train(10, lstm_model, optimizer, F.nll_loss, text, gen_text = False)\n",
    "    \n",
    "    return lstm_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building lstm mobel\n",
      "Start train loop with\n",
      "Training epoch  0\n",
      "Debias loss:  2.401770835482364 Avg loss:  2.4089311845250956\n",
      "Text after  0 iteration\n",
      "Training epoch  1\n",
      "Debias loss:  2.1358053154811736 Avg loss:  2.1421727470946457\n",
      "Text after  1 iteration\n",
      "Training epoch  2\n",
      "Debias loss:  2.004041725635613 Avg loss:  2.010016333220881\n",
      "Text after  2 iteration\n",
      "Training epoch  3\n",
      "Debias loss:  1.920225619380205 Avg loss:  1.9259503477649575\n",
      "Text after  3 iteration\n",
      "Training epoch  4\n",
      "Debias loss:  1.8652694750280105 Avg loss:  1.8708303638116692\n",
      "Text after  4 iteration\n",
      "Training epoch  5\n",
      "Debias loss:  1.8217622587829083 Avg loss:  1.8271934404148313\n",
      "Text after  5 iteration\n",
      "Training epoch  6\n",
      "Debias loss:  1.7889114938935466 Avg loss:  1.794244738228766\n",
      "Text after  6 iteration\n",
      "Training epoch  7\n",
      "Debias loss:  1.7618841956059061 Avg loss:  1.7671368640233225\n",
      "Text after  7 iteration\n",
      "Training epoch  8\n",
      "Debias loss:  1.7391133360332331 Avg loss:  1.7442981181643584\n",
      "Text after  8 iteration\n",
      "Training epoch  9\n",
      "Debias loss:  1.7224103206647814 Avg loss:  1.7275453064463393\n",
      "Text after  9 iteration\n",
      "Start train loop with\n",
      "Training epoch  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = read_file(NITZ_TRN_FILE)\n",
    "idx2text, text2idx = build_vocab(text)\n",
    "lstm_model = build_and_train(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text using LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_next_n(lstm_model, 'I am ', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model on comments from NewYork times articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = 'text-generation/ny_articles'\n",
    "\n",
    "df1 = pd.read_csv(DATA_PATH+\"/CommentsApril2018.csv.gz\")\n",
    "df2 = pd.read_csv(DATA_PATH+\"/CommentsFeb2018.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.concat([df1, df2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COMMENTS_DATA_FILE = './text-generation/comments.txt'\n",
    "\n",
    "\n",
    "def extract_comments(df, dest_file):\n",
    "    comments = list(df['commentBody'])\n",
    "    comments_text = \" \".join(comments)\n",
    "    text_file = open(dest_file, \"w\")\n",
    "    text_file.write(comments_text)\n",
    "    text_file.close()\n",
    "\n",
    "extract_comments(df, COMMENTS_DATA_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./text-generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_TRN_FILE = \"./text-generation/comments.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'COMMENTS_TRN_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e5d16a2aa300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMMENTS_TRN_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0midx2text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcomments_lstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'COMMENTS_TRN_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "text = read_file(COMMENTS_TRN_FILE)\n",
    "idx2text, text2idx = build_vocab(text)\n",
    "comments_lstm_model = build_and_train(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_next_n(lstm_model, 'I am ', 1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3_rl)",
   "language": "python",
   "name": "conda_python3_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
