{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recognize named entities on Twitter with LSTMs\n",
    "\n",
    "In this workshop, you will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities from Twitter.\n",
    "\n",
    "For example, we want to extract persons' and organizations' names from the text. Than for the input text:\n",
    "\n",
    "    Donald Trump is the president of the United States\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "    B-PER I-PER   O O O O O   B-COUNTRY  I-COUNTRY\n",
    "\n",
    "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
    "\n",
    "A solution of the task will be based on neural networks, particularly, on Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs).\n",
    "\n",
    "### Libraries\n",
    "\n",
    "For this task you will need the following libraries:\n",
    " - [Pytorch](https://pytorch.org/docs/stable/index.html) — an open-source software library for Machine Intelligence.\n",
    " - [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "\n",
    "Add tutorial link to Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: describe the task in details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "train.txt\n",
      "**************************************************\n",
      "test.txt\n",
      "**************************************************\n",
      "validation.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from common.evaluation import precision_recall_f1\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import common.workshop as workshop\n",
    "\n",
    "workshop.download_ner_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup execution device\n",
    "\n",
    "Note: since this is hevy computational task, we need to use GPU, make sure that the cell below outputs 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Execution device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.txt  train.txt  validation.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./ner/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"ner\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file \n",
    "\n",
    "Read data from file and change replace users and urls with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "\n",
    "            if (token.startswith('@')):\n",
    "                token = '<USR>'\n",
    "            elif token.startswith('http://') or token.startswith('https://'):\n",
    "                token = '<URL>'\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can load three separate parts of the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_tokens, train_tags = read_data(DATA_DIR + '/train.txt')\n",
    "validation_tokens, validation_tags = read_data(DATA_DIR + '/validation.txt')\n",
    "test_tokens, test_tags = read_data(DATA_DIR + '/test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['RT', '<USR>', ':', 'Online', 'ticket', 'sales', 'for', 'Ghostland', 'Observatory', 'extended', 'until', '6', 'PM', 'EST', 'due', 'to', 'high', 'demand', '.', 'Get', 'them', 'before', 'they', 'sell', 'out', '...'], ['Apple', 'MacBook', 'Pro', 'A1278', '13.3', '\"', 'Laptop', '-', 'MD101LL/A', '(', 'June', ',', '2012', ')', '-', 'Full', 'read', 'by', 'eBay', '<URL>', '<URL>']]\n"
     ]
    }
   ],
   "source": [
    "print (train_tokens[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lest take a look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\tO\n",
      "<USR>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "\n",
      "Apple\tB-product\n",
      "MacBook\tI-product\n",
      "Pro\tI-product\n",
      "A1278\tI-product\n",
      "13.3\tI-product\n",
      "\"\tI-product\n",
      "Laptop\tI-product\n",
      "-\tI-product\n",
      "MD101LL/A\tI-product\n",
      "(\tO\n",
      "June\tO\n",
      ",\tO\n",
      "2012\tO\n",
      ")\tO\n",
      "-\tO\n",
      "Full\tO\n",
      "read\tO\n",
      "by\tO\n",
      "eBay\tB-company\n",
      "<URL>\tO\n",
      "<URL>\tO\n",
      "\n",
      "Happy\tO\n",
      "Birthday\tO\n",
      "<USR>\tO\n",
      "!\tO\n",
      "May\tO\n",
      "Allah\tB-person\n",
      "s.w.t\tO\n",
      "bless\tO\n",
      "you\tO\n",
      "with\tO\n",
      "goodness\tO\n",
      "and\tO\n",
      "happiness\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(3):\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "    # Create mappings from tokens (or tags) to indices and vice versa.\n",
    "    # Add special tokens (or tags) to the dictionaries.\n",
    "    # The first special token must have index 0.\n",
    "    \n",
    "    # Mapping tok2idx should contain each token or tag only once. \n",
    "    # To do so, you should extract unique tokens/tags from the tokens_or_tags variable\n",
    "    # and then index them (for example, you can add them into the list idx2tok\n",
    "    # and for each token/tag save the index into tok2idx).\n",
    "    \n",
    "    idx = 0\n",
    "    for token in special_tokens:\n",
    "        idx2tok.append(token)\n",
    "        tok2idx[token]=idx\n",
    "        idx+=1\n",
    "    \n",
    "    for token_list in tokens_or_tags:\n",
    "        for token in token_list:\n",
    "            if token not in idx2tok:\n",
    "                idx2tok.append(token)\n",
    "                tok2idx[token]=idx\n",
    "                idx+=1\n",
    "    \n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next additional functions will help you to create the mapping between tokens and ids for a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token. It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32) (10, 32)\n"
     ]
    }
   ],
   "source": [
    "# check the generator\n",
    "\n",
    "batch_size= 10\n",
    "x,y, _ = next(batches_generator(batch_size, train_tokens, train_tags))\n",
    "\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurrent neural network\n",
    "\n",
    "This is the most important part of the assignment. Here we will specify the network architecture based on Pytorch building blocks. It's fun and easy as a lego constructor! We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be used on top to perform tag classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: explain what architecture needs to be built, and provide documentation to corresponding blocks\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLstm(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_hidden, n_output):\n",
    "        super(BiLstm, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        self.embed_layer = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.lstm_layer = nn.LSTM(embed_size, n_hidden,\n",
    "                                  num_layers = 2, batch_first = True, \n",
    "                                  bidirectional = True)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(2*n_hidden, n_output)\n",
    "        \n",
    "\n",
    "    # input_tensor - shape (batch_size, seq_length)\n",
    "    # hidden - pair of tensors of shape (batch_size, hidden_size)\n",
    "    def forward(self, input_tensor, seq_length, batch_size):\n",
    "\n",
    "        # e_tensor - (batch_size, seq_length, embed_size)\n",
    "        e_tensor = self.embed_layer(input_tensor)\n",
    "              \n",
    "        # execute lstm layer\n",
    "        lstm_out, _ = self.lstm_layer(e_tensor)\n",
    "\n",
    "        \n",
    "        # transfor output to the 2d matrix of shape (batch_size * seq_length, 2*hidden_size)\n",
    "        # since it is bidirectional network there is double size of hidden parameters\n",
    "        output_tensor = lstm_out.contiguous().view(-1, 2 * self.n_hidden)\n",
    "        # execute linear layer\n",
    "        output_tensor = self.linear_layer(output_tensor)\n",
    "        \n",
    "        return output_tensor\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constructs pytorch tensor from numpy array\n",
    "def construct_pytorch_tensor(numpy_tensor):\n",
    "    tensor = torch.from_numpy(numpy_tensor).long()\n",
    "    # Send tensor to the device\n",
    "    tensor = tensor.to(device)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def construct_model(vocab_size, embed_size, hidden_size, output_size):\n",
    "    bi_nn = BiLstm(vocab_size=vocab_size, embed_size=embed_size, \n",
    "                 n_hidden=hidden_size, n_output=output_size)\n",
    "    bi_nn = bi_nn.to(device)\n",
    "    return bi_nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network\n",
    "\n",
    "lest test the created network. Play with parameters to see how do the affect input and output tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84, 21])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "vocab_size = len(idx2token)\n",
    "n_tags = len(idx2tag)\n",
    "n_input = 100\n",
    "n_hidden = 40\n",
    "batch_size = 4\n",
    "embed_size = 30\n",
    "\n",
    "x,y, l = next(batches_generator(batch_size, train_tokens, train_tags))\n",
    "\n",
    "# seq length\n",
    "model_seq_length = x.shape[1]\n",
    "\n",
    "input_tensor = construct_pytorch_tensor(x) # construct input tensor\n",
    "\n",
    "target_tensor = construct_pytorch_tensor(y) # construct target tensor\n",
    "\n",
    "bi_nn = construct_model(vocab_size, embed_size, n_hidden, n_tags) # init model\n",
    "\n",
    "output_tensor = bi_nn(input_tensor, model_seq_length, batch_size) # execute forward\n",
    "print(output_tensor.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "below defined evaluation functions.\n",
    "\n",
    "We use precision/recall metric and F1 score to determine the performance.\n",
    "\n",
    "Additional resources:\n",
    "\n",
    "[precision/recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n",
    "[f1score](https://skymind.ai/wiki/accuracy-precision-recall-f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_on_data(model, tokens, tags):\n",
    "    y_true_indx = []\n",
    "    y_pred_indx = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i, (x, y, _) in enumerate(batches_generator(batch_size, tokens, tags)):\n",
    "        input_tensor = torch.from_numpy(x).long().to(device)\n",
    "        taget_tensor = torch.from_numpy(y).long().to(device)\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        seq_length = x.shape[1]\n",
    "\n",
    "        output_tensor = model(input_tensor, seq_length, batch_size)\n",
    "        output_tensor = F.softmax(output_tensor, dim = 1)\n",
    "        _, output_inds = output_tensor.max(dim = 1)\n",
    "        \n",
    "        output_inds = output_inds.long()\n",
    "        \n",
    "        y_true_indx_batch = list(taget_tensor.cpu().numpy().reshape(-1))\n",
    "        y_pred_indx_batch = list(output_inds.cpu().detach().numpy().reshape(-1))\n",
    "        y_true_indx = y_true_indx + y_true_indx_batch\n",
    "        y_pred_indx = y_pred_indx + y_pred_indx_batch\n",
    "        \n",
    "    y_true = [idx2tag[idx] for idx in y_true_indx]\n",
    "    y_pred = [idx2tag[idx] for idx in y_pred_indx]\n",
    "    \n",
    "    precision_recall_f1(y_true, y_pred, print_results=True, short_report=True)\n",
    "    \n",
    "def evaluate(model):\n",
    "    # For evaluation we do not need to update gradients and compute derivatives\n",
    "    with torch.no_grad():\n",
    "        print('Evaluation on train set')\n",
    "        evaluate_on_data(model, train_tokens, train_tags)\n",
    "        print('Evaluation on validation set')\n",
    "        evaluate_on_data(model, validation_tokens, validation_tags)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop\n",
    "\n",
    "Below defined the train loop for a single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer, loss_fn, batch_size = 32):\n",
    "    for batch_num, (input_data, target_data, _) in enumerate(batches_generator(batch_size, train_tokens, train_tags)):\n",
    "        \n",
    "        # The last batch can be smaller than others\n",
    "        train_batch_size = input_data.shape[0]\n",
    "        # Since each batch has different sequence length we need to update the variable for each batch\n",
    "        train_seq_length = input_data.shape[1]\n",
    "\n",
    "        input_tensor = construct_pytorch_tensor(input_data)\n",
    "        target_tensor = construct_pytorch_tensor(target_data)\n",
    "        \n",
    "        # zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the output sequence from the input and the initial hidden and cell states\n",
    "        output_tensor = model(input_tensor, train_seq_length, train_batch_size).to(device)\n",
    "    \n",
    "        # we need to calculate the loss across all batches, so we have to flat the targets tensor\n",
    "#         pdb.set_trace()\n",
    "        target_tensor = target_tensor.view((train_seq_length*train_batch_size, -1)).squeeze(dim=1)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(output_tensor, target_tensor)\n",
    "\n",
    "        # calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the parameters of the model\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "Evaluation on train set\n",
      "processed 179408 tokens with 4489 phrases; found: 2302 phrases; correct: 716.\n",
      "\n",
      "precision:  31.10%; recall:  15.95%; F1:  21.09\n",
      "\n",
      "Evaluation on validation set\n",
      "processed 22020 tokens with 537 phrases; found: 211 phrases; correct: 61.\n",
      "\n",
      "precision:  28.91%; recall:  11.36%; F1:  16.31\n",
      "\n",
      "Starting epoch:  1\n",
      "Evaluation on train set\n",
      "processed 180340 tokens with 4489 phrases; found: 4149 phrases; correct: 2243.\n",
      "\n",
      "precision:  54.06%; recall:  49.97%; F1:  51.93\n",
      "\n",
      "Evaluation on validation set\n",
      "processed 22308 tokens with 537 phrases; found: 345 phrases; correct: 145.\n",
      "\n",
      "precision:  42.03%; recall:  27.00%; F1:  32.88\n",
      "\n",
      "Starting epoch:  2\n",
      "Evaluation on train set\n",
      "processed 181188 tokens with 4489 phrases; found: 4493 phrases; correct: 3298.\n",
      "\n",
      "precision:  73.40%; recall:  73.47%; F1:  73.44\n",
      "\n",
      "Evaluation on validation set\n",
      "processed 22444 tokens with 537 phrases; found: 436 phrases; correct: 167.\n",
      "\n",
      "precision:  38.30%; recall:  31.10%; F1:  34.33\n",
      "\n",
      "Starting epoch:  3\n",
      "Evaluation on train set\n",
      "processed 181006 tokens with 4489 phrases; found: 4607 phrases; correct: 4014.\n",
      "\n",
      "precision:  87.13%; recall:  89.42%; F1:  88.26\n",
      "\n",
      "Evaluation on validation set\n",
      "processed 22064 tokens with 537 phrases; found: 462 phrases; correct: 175.\n",
      "\n",
      "precision:  37.88%; recall:  32.59%; F1:  35.04\n",
      "\n",
      "Starting epoch:  4\n",
      "Evaluation on train set\n",
      "processed 179970 tokens with 4489 phrases; found: 4560 phrases; correct: 4336.\n",
      "\n",
      "precision:  95.09%; recall:  96.59%; F1:  95.83\n",
      "\n",
      "Evaluation on validation set\n",
      "processed 22356 tokens with 537 phrases; found: 549 phrases; correct: 190.\n",
      "\n",
      "precision:  34.61%; recall:  35.38%; F1:  34.99\n",
      "\n",
      "Starting epoch:  5\n",
      "Evaluation on train set\n",
      "processed 179423 tokens with 4489 phrases; found: 4509 phrases; correct: 4409.\n",
      "\n",
      "precision:  97.78%; recall:  98.22%; F1:  98.00\n",
      "\n",
      "Evaluation on validation set\n",
      "processed 22392 tokens with 537 phrases; found: 546 phrases; correct: 179.\n",
      "\n",
      "precision:  32.78%; recall:  33.33%; F1:  33.06\n",
      "\n",
      "Starting epoch:  6\n",
      "Evaluation on train set\n",
      "processed 180808 tokens with 4489 phrases; found: 4518 phrases; correct: 4436.\n",
      "\n",
      "precision:  98.19%; recall:  98.82%; F1:  98.50\n",
      "\n",
      "Evaluation on validation set\n",
      "processed 22576 tokens with 537 phrases; found: 535 phrases; correct: 189.\n",
      "\n",
      "precision:  35.33%; recall:  35.20%; F1:  35.26\n",
      "\n",
      "Starting epoch:  7\n",
      "Evaluation on train set\n",
      "processed 180104 tokens with 4489 phrases; found: 4491 phrases; correct: 4452.\n",
      "\n",
      "precision:  99.13%; recall:  99.18%; F1:  99.15\n",
      "\n",
      "Evaluation on validation set\n",
      "processed 22356 tokens with 537 phrases; found: 451 phrases; correct: 193.\n",
      "\n",
      "precision:  42.79%; recall:  35.94%; F1:  39.07\n",
      "\n",
      "Starting epoch:  8\n",
      "Evaluation on train set\n",
      "processed 179772 tokens with 4489 phrases; found: 4500 phrases; correct: 4461.\n",
      "\n",
      "precision:  99.13%; recall:  99.38%; F1:  99.25\n",
      "\n",
      "Evaluation on validation set\n",
      "processed 22480 tokens with 537 phrases; found: 470 phrases; correct: 197.\n",
      "\n",
      "precision:  41.91%; recall:  36.69%; F1:  39.13\n",
      "\n",
      "Starting epoch:  9\n",
      "Evaluation on train set\n",
      "processed 179688 tokens with 4489 phrases; found: 4483 phrases; correct: 4466.\n",
      "\n",
      "precision:  99.62%; recall:  99.49%; F1:  99.55\n",
      "\n",
      "Evaluation on validation set\n",
      "processed 22464 tokens with 537 phrases; found: 512 phrases; correct: 201.\n",
      "\n",
      "precision:  39.26%; recall:  37.43%; F1:  38.32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "\n",
    "hidden_size = 256\n",
    "batch_size = 32\n",
    "embed_size = 256\n",
    "n_tags = len(idx2tag)\n",
    "vocab_size = len(idx2token)\n",
    "\n",
    "\n",
    "model = construct_model(vocab_size, embed_size, hidden_size, n_tags) # init model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005) # Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss() # Cross entropy loss\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print('Starting epoch: ', epoch)\n",
    "    train(model, optimizer, criterion, batch_size)\n",
    "\n",
    "    evaluate(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3_rl)",
   "language": "python",
   "name": "conda_python3_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
